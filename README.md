# 100 Days of Deep Learning

Welcome to 100 Days of Deep Learning! This repository documents important concepts from beginner to advanced levels.

## ‚úÖ Topics üìö

- [x] 1. 100 Days of Deep Learning
- [ ] 2. What is Deep Learning? And Deep Learning v/s Machine Learning
- [ ] 3. Types of Neural Networks, History of Deep Learning And Applications of Deep Learning
- [ ] 4. What is a Perceptron?, Perceptron v/s Neuron And Perceptron Geometric Intuition
- [ ] 5. Perceptron Tricks And How to Train a Perceptron
- [ ] 6. Perceptron Loss Function [Hinge Loss, Binary Cross Entropy] | Sigmoid Function
- [ ] 7. Problems with Perceptron
- [ ] 8. MLP Notation
- [ ] 9. Multi-Layer Perceptron Intuition
- [ ] 10. Forward Propagation And How a Neural Network Predicts Output
- [ ] 11. Customer Churn Prediction using ANN
- [ ] 12. Handwritten Digit Classification using ANN on MNIST Dataset
- [ ] 13. Graduate Admission Prediction using ANN
- [ ] 14. Loss Functions in Deep Learning
- [ ] 15. Backpropagation Part 1: The What?
- [ ] 16. Backpropagation Part 2: The How?
- [ ] 17. Backpropagation Part 3: The Why?
- [ ] 18. MLP Memoization
- [ ] 19. Gradient Descent in Neural Networks And Batch v/s Stochastic v/s Mini-Batch Gradient Descent
- [ ] 20. Vanishing Gradient Problem And Exploding Gradient Problem
- [ ] 21. How to Improve the Performance of a Neural Network
- [ ] 22. Early Stopping in Neural Networks
- [ ] 23. Data Scaling in Neural Networks And Feature Scaling in ANN
- [ ] 24. Dropout Layer in Deep Learning
- [ ] 25. Dropout Layers in ANN [Regression, Classification]
- [ ] 26. Regularization in Deep Learning [L2 Regularization, L1 Regularization] | Weight Decay
- [ ] 27. Activation Functions in Deep Learning [Sigmoid, Tanh, and ReLU]
- [ ] 28. ReLU Variants [Leaky ReLU, Parametric ReLU, ELU, SELU]
- [ ] 29. Weight Initialization Techniques
- [ ] 30. Xavier/Glorot and He Weight Initialization
- [ ] 31. Batch Normalization
- [ ] 32. Optimizers
- [ ] 33. Exponentially Weighted Moving Average
- [ ] 34. SGD with Momentum
- [ ] 35. Nesterov Accelerated Gradient (NAG)
- [ ] 36. AdaGrad
- [ ] 37. RMSProp
- [ ] 38. Adam Optimizer
- [ ] 39. Keras Tuner For Hyperparameter Tuning
- [ ] 40. What is a Convolutional Neural Network (CNN) | Intuition
- [ ] 41. CNN v/s Visual Cortex, The Famous Cat Experiment And History of CNN
- [ ] 42. Convolution Operation in CNN
- [ ] 43. Padding & Strides in CNN
- [ ] 44. Pooling Layer in CNN
- [ ] 45. CNN Architecture - LeNet-5
- [ ] 46. Comparing CNN v/s ANN
- [ ] 47. Backpropagation in CNN - Part 1
- [ ] 48. Backpropagation in CNN - Part 2 [Convolution, Maxpooling, and Flatten Layers]
- [ ] 49. Cat v/s Dog Image Classification Project
- [ ] 50. Data Augmentation
- [ ] 51. Pretrained Models in CNN | ImageNet Dataset | ILSVRC
- [ ] 52. Visualizing CNN Filters and Feature Maps
- [ ] 53. Transfer Learning in Keras And Fine Tuning v/s Feature Extraction
- [ ] 54. Keras Functional Model And Building Non-linear Neural Networks
- [ ] 55. Why RNNs are Needed And RNNs v/s ANNs
- [ ] 56. Recurrent Neural Network, Forward Propagation And Architecture
- [ ] 57. RNN Sentiment Analysis
- [ ] 58. Types of RNN - Many-to-Many, One-to-Many, Many-to-One
- [ ] 59. Backpropagation Through Time in RNN
- [ ] 60. Problems with RNN
- [ ] 61. LSTM Part 1: The What?
- [ ] 62. LSTM Part 2: The How?
- [ ] 63. LSTM Part 3: Next Word Predictor
- [ ] 64. Gated Recurrent Unit (GRU)
- [ ] 65. Deep RNNs, Stacked RNNs, Stacked LSTMs And Stacked GRUs
- [ ] 66. Bidirectional RNN, BiLSTM, Bidirectional LSTM, Bidirectional GRU
- [ ] 67. History of Large Language Models (LLMs) - From LSTMs to ChatGPT
- [ ] 68. Encoder-Decoder And Sequence-to-Sequence Architecture
- [ ] 69. Attention Mechanism, Seq2Seq Networks And Encoder-Decoder Architecture
- [ ] 70. Bahdanau Attention v/s Luong Attention
- [ ] 71. Introduction to Transformers
- [ ] 72. What is Self-Attention
- [ ] 73. Self-Attention in Transformers
- [ ] 74. Scaled Dot Product Attention And Why Scale Self-Attention?
- [ ] 75. Self-Attention Geometric Intuition And Visualization
- [ ] 76. Why is Self-Attention Called "Self"? And Self-Attention v/s Luong Attention
- [ ] 77. Multi-head Attention in Transformers
- [ ] 78. Positional Encoding in Transformers
- [ ] 79. Layer Normalization in Transformers And Layer Norm v/s Batch Norm
- [ ] 80. Transformer Architecture - Encoder Architecture
- [ ] 81. Masked Self-Attention, Masked Multi-head Attention And Transformer Decoder
- [ ] 82. Cross Attention in Transformers
- [ ] 83. Transformer Decoder Architecture
- [ ] 84. Transformer Inference And How Inference is Done in Transformers

## üõ†Ô∏è Technologies & Tools

- **Programming Languages:** Python
- **Deep Learning Frameworks:** TensorFlow, PyTorch, Keras
- **Data Science Libraries:** NumPy, Pandas, Matplotlib, Seaborn
- **Development Environment:** Jupyter Notebook, Google Colab, VS Code
- **Version Control:** Git & GitHub

### Useful Links

- [TensorFlow Documentation](https://www.tensorflow.org/)
- [PyTorch Documentation](https://pytorch.org/)
- [Papers With Code](https://paperswithcode.com/)
- [Towards Data Science](https://towardsdatascience.com/)

‚≠ê **Star this repository if you find it helpful!**

**Happy Learning! üöÄ**
